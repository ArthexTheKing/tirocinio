{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import Env\n",
    "from gymnasium.spaces import MultiDiscrete, Discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommenderSysEnv(Env):\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        super(RecommenderSysEnv, self).__init__()\n",
    "        # Carico il dataset della sperimentazione\n",
    "        self.__dataset = df\n",
    "        # Definisco spazio stato-azione\n",
    "        self.observation_space = MultiDiscrete([5, 5, 5, 5, 5, 5])\n",
    "        self.action_space = Discrete(5, start = 1)\n",
    "\n",
    "    def reset(self):\n",
    "        # Genero uno stato iniziale casuale\n",
    "        self.__n_steps = 0\n",
    "        self.__stato_attuale = np.random.randint(1, 6, size=6)\n",
    "\n",
    "        return self.__stato_attuale.tolist(), {}\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Aggiorno il numero di step intrapresi\n",
    "        self.__n_steps += 1\n",
    "        # Calcolo la ricompensa\n",
    "        reward = self.__reward_function(action)\n",
    "        # Verifico la terminazione dell'epoca\n",
    "        done = True if self.__n_steps >= 500 else False\n",
    "        # Calcolo lo stato Successivo\n",
    "        self.__stato_attuale = np.append(self.__stato_attuale, action)[1:]\n",
    "        \n",
    "        return self.__stato_attuale, reward, done, {}\n",
    "    \n",
    "    def __reward_function(self, action):\n",
    "        match_state_action = self.__dataset['bmi_variation'].loc[\n",
    "            (self.__dataset['sb1'] == self.__stato_attuale[0]) &\n",
    "            (self.__dataset['sb2'] == self.__stato_attuale[1]) &\n",
    "            (self.__dataset['sb3'] == self.__stato_attuale[2]) &\n",
    "            (self.__dataset['sb4'] == self.__stato_attuale[3]) &\n",
    "            (self.__dataset['sb5'] == self.__stato_attuale[4]) &\n",
    "            (self.__dataset['sb6'] == self.__stato_attuale[5]) &\n",
    "            (self.__dataset['action_bin'] == action)\n",
    "        ]\n",
    "\n",
    "        bmi_variaton = match_state_action.min()\n",
    "\n",
    "        if bmi_variaton > 0:\n",
    "            return 1\n",
    "        elif bmi_variaton < 0:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent():\n",
    "\n",
    "    def __init__(self, alpha = 0.3, gamma = 0.9):\n",
    "        self.__alpha = alpha\n",
    "        self.__gamma = gamma\n",
    "        self.__Q = {}\n",
    "\n",
    "        # Inizializzo la Q_table\n",
    "        for i in range(111111, 555556):\n",
    "            self.__Q[str(i)] = np.zeros(5, dtype=float)\n",
    "\n",
    "    def get_q_table(self):\n",
    "        return self.__Q\n",
    "\n",
    "    def scegli_azione(self, stato):\n",
    "        q_key = self.__converti_stato_in_chiave(stato)\n",
    "        return np.argmax(self.__Q[q_key]) + 1\n",
    "    \n",
    "    def aggiorna_q_table(self, stato, azione, ricompensa, stato_successivo):\n",
    "        self.__Q[self.__converti_stato_in_chiave(stato)][azione - 1] += self.__alpha * (ricompensa + self.__gamma * np.max(self.__Q[self.__converti_stato_in_chiave(stato_successivo)]) - self.__Q[self.__converti_stato_in_chiave(stato)][azione - 1])\n",
    "\n",
    "    @staticmethod\n",
    "    def __converti_stato_in_chiave(stato):\n",
    "        return ''.join(map(str, stato))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_phase(env: RecommenderSysEnv, agent: QLearningAgent):\n",
    "    for episode in range(1000):\n",
    "        print('Episodio {}'.format(episode + 1))\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.scegli_azione(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.aggiorna_q_table(state, action, reward, next_state)\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_phase(X_test: pd.DataFrame, agent: QLearningAgent):\n",
    "    w_match_gain, w_match_loss, w_miss_gain, w_miss_loss = 0, 0, 0, 0\n",
    "\n",
    "    for _, row in X_test.iterrows():\n",
    "        stato = map(int, row.iloc[:6])\n",
    "        azione_predetta = agent.scegli_azione(stato)\n",
    "        azione_effettiva = row.iloc[-1]\n",
    "        variazione_bmi = row.iloc[-2]\n",
    "\n",
    "        # Verifica validitÃ  predizione\n",
    "        if (azione_predetta == azione_effettiva) and (variazione_bmi > 0):\n",
    "            w_match_gain += 1\n",
    "        elif (azione_predetta != azione_effettiva) and (variazione_bmi > 0):\n",
    "            w_miss_gain += 1\n",
    "        elif (azione_predetta == azione_effettiva) and (variazione_bmi <= 0):\n",
    "            w_match_loss += 1\n",
    "        elif (azione_predetta != azione_effettiva) and (variazione_bmi <= 0):\n",
    "            w_miss_loss += 1\n",
    "    \n",
    "    # Calcolo la precisione del modello\n",
    "    precisione = w_match_loss / (w_match_loss + w_miss_loss)\n",
    "\n",
    "    # Calcolo l'accuratezza del modello\n",
    "    accuratezza = (w_match_gain + w_match_loss) / (w_match_loss + w_match_gain + w_miss_loss + w_miss_gain)\n",
    "\n",
    "    return precisione, accuratezza\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    df = pd.read_csv('../../dataset/dataset_cbr.csv')\n",
    "\n",
    "    X_train, X_test = train_test_split(df, test_size=0.1, random_state=46)\n",
    "\n",
    "    env = RecommenderSysEnv(X_train)\n",
    "    agent = QLearningAgent()\n",
    "\n",
    "    training_phase(env, agent)\n",
    "\n",
    "    pr, acc = test_phase(X_test, agent)\n",
    "\n",
    "    display('Accuratezza: {:.2f}'.format(acc))\n",
    "    display('Precisione: {:.2f}'.format(pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
